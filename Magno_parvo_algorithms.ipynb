{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nelsonalbertohj/Magno-Parvo-CNN/blob/main/Magno_parvo_algorithms.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow_addons"
      ],
      "metadata": {
        "id": "B8jN2PimUzk6",
        "outputId": "0105dbc2-49dc-4cb4-c56f-44bac1c23f40",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "B8jN2PimUzk6",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow_addons in /usr/local/lib/python3.7/dist-packages (0.16.1)\n",
            "Requirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow_addons) (2.7.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "273e5c47",
      "metadata": {
        "scrolled": false,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "273e5c47",
        "outputId": "f4167caa-a57a-4425-bf96-52e8533de283"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tensorflow version:  2.8.0\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import os\n",
        "import numpy as np\n",
        "import pathlib\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "import keras\n",
        "import time\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Conv2D, MaxPool2D , Flatten, Average\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "import tensorflow_addons as tfa\n",
        "print(\"Tensorflow version: \",tf.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FTxP_p0fZrGX",
        "outputId": "048adb7c-bf4c-4ddb-bd12-2b5ccbd4c3b0"
      },
      "id": "FTxP_p0fZrGX",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "61f265cf",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "61f265cf",
        "outputId": "546ab85d-4b42-4c55-8b05-50922cd81815"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 11636 files belonging to 10 classes.\n",
            "Using 9309 files for training.\n",
            "Found 11636 files belonging to 10 classes.\n",
            "Using 2327 files for validation.\n"
          ]
        }
      ],
      "source": [
        "root = \"/content/drive/MyDrive/Vision Dataset/Imagenet-10-1500-splits/\"\n",
        "loc = root + \"train\"\n",
        "data_dir = pathlib.Path(loc)\n",
        "\n",
        "batch_size = 32\n",
        "img_height = 224\n",
        "img_width = 224\n",
        "\n",
        "#Training set\n",
        "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "  data_dir,\n",
        "  validation_split=0.2,\n",
        "  subset=\"training\",\n",
        "  seed=123,\n",
        "  label_mode = 'categorical',\n",
        "  image_size=(img_height, img_width),\n",
        "  batch_size=batch_size)\n",
        "\n",
        "#Testing set\n",
        "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "  data_dir,\n",
        "  validation_split=0.2,\n",
        "  subset=\"validation\",\n",
        "  seed=123,\n",
        "  label_mode = 'categorical',\n",
        "  image_size=(img_height, img_width),\n",
        "  batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "498b8dd1",
      "metadata": {
        "id": "498b8dd1"
      },
      "outputs": [],
      "source": [
        "def img_transforms(data,label):\n",
        "    normalize_img = tf.keras.layers.Rescaling(1./255)\n",
        "    color_norm = normalize_img(data)\n",
        "    gray_img = tf.image.rgb_to_grayscale(data)\n",
        "    gray_norm_img = normalize_img(gray_img)\n",
        "    gray_norm_img = tfa.image.gaussian_filter2d(gray_norm_img,filter_shape=(10,10),sigma=5.0)\n",
        "    concat_img = tf.concat([color_norm, gray_norm_img],axis=-1)\n",
        "    return concat_img,label\n",
        "\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "train_ds_preprocessed = train_ds.map(img_transforms)\n",
        "val_ds_preprocessed = val_ds.map(img_transforms)\n",
        "\n",
        "#NOTE: THE CASHE ON DISK IS ONLY NECESSARY IF HAVING ISSUES WITH IMAGE LOADING BOTTLENECK\n",
        "CASHE_Train = f\"{root}CASHE_Train_Blur_50_1500\"\n",
        "CASHE_Val = f\"{root}CASHE_Val_Blur_50_1500\"\n",
        "train_ds_preprocessed = train_ds_preprocessed.cache(CASHE_Train).prefetch(buffer_size=AUTOTUNE)\n",
        "val_ds_preprocessed = val_ds_preprocessed.cache(CASHE_Val).prefetch(buffer_size=AUTOTUNE)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Used to create CASHE in local disk by loading all images\n",
        "idx = 0\n",
        "start_time = time.time()\n",
        "for t in train_ds_preprocessed:\n",
        "  print(\"time to get object: \", time.time()-start_time)\n",
        "  idx += 1\n",
        "  start_time = time.time()"
      ],
      "metadata": {
        "id": "X7cRFBywFpP8",
        "outputId": "6c23476d-bf81-44ec-fa55-9f8914cb0dbd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "X7cRFBywFpP8",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time to get object:  33.0582971572876\n",
            "time to get object:  12.263034343719482\n",
            "time to get object:  14.473114728927612\n",
            "time to get object:  12.226741313934326\n",
            "time to get object:  16.13148045539856\n",
            "time to get object:  12.200576305389404\n",
            "time to get object:  11.591894388198853\n",
            "time to get object:  12.674214601516724\n",
            "time to get object:  13.647250652313232\n",
            "time to get object:  11.113381147384644\n",
            "time to get object:  12.316364049911499\n",
            "time to get object:  13.428166151046753\n",
            "time to get object:  12.962493896484375\n",
            "time to get object:  13.708700895309448\n",
            "time to get object:  11.886165857315063\n",
            "time to get object:  13.076969623565674\n",
            "time to get object:  14.407016515731812\n",
            "time to get object:  13.170894145965576\n",
            "time to get object:  10.560872793197632\n",
            "time to get object:  13.154829263687134\n",
            "time to get object:  12.825519323348999\n",
            "time to get object:  12.01262640953064\n",
            "time to get object:  13.416741132736206\n",
            "time to get object:  12.664159297943115\n",
            "time to get object:  13.163422584533691\n",
            "time to get object:  11.887457847595215\n",
            "time to get object:  13.764501333236694\n",
            "time to get object:  11.47451114654541\n",
            "time to get object:  12.832497358322144\n",
            "time to get object:  12.39275050163269\n",
            "time to get object:  13.162086486816406\n",
            "time to get object:  11.255962610244751\n",
            "time to get object:  11.51087737083435\n",
            "time to get object:  10.923557043075562\n",
            "time to get object:  11.959886312484741\n",
            "time to get object:  13.648026466369629\n",
            "time to get object:  12.26061224937439\n",
            "time to get object:  12.446887969970703\n",
            "time to get object:  12.049174070358276\n",
            "time to get object:  13.079254627227783\n",
            "time to get object:  11.775855541229248\n",
            "time to get object:  12.296628952026367\n",
            "time to get object:  11.995187282562256\n",
            "time to get object:  11.548802852630615\n",
            "time to get object:  11.259289979934692\n",
            "time to get object:  11.863420724868774\n",
            "time to get object:  11.99742579460144\n",
            "time to get object:  11.203981637954712\n",
            "time to get object:  13.012274265289307\n",
            "time to get object:  13.59084939956665\n",
            "time to get object:  12.033011436462402\n",
            "time to get object:  12.41971468925476\n",
            "time to get object:  12.076093435287476\n",
            "time to get object:  12.998363256454468\n",
            "time to get object:  13.036859273910522\n",
            "time to get object:  10.28371810913086\n",
            "time to get object:  12.528733730316162\n",
            "time to get object:  12.788887977600098\n",
            "time to get object:  11.84778094291687\n",
            "time to get object:  11.595199584960938\n",
            "time to get object:  12.500590562820435\n",
            "time to get object:  12.32804560661316\n",
            "time to get object:  11.599068403244019\n",
            "time to get object:  12.541652917861938\n",
            "time to get object:  12.472028732299805\n",
            "time to get object:  12.078728199005127\n",
            "time to get object:  13.310323238372803\n",
            "time to get object:  13.47719669342041\n",
            "time to get object:  10.406190872192383\n",
            "time to get object:  12.45281171798706\n",
            "time to get object:  12.292632579803467\n",
            "time to get object:  11.945407629013062\n",
            "time to get object:  11.017317771911621\n",
            "time to get object:  12.971832036972046\n",
            "time to get object:  11.454123258590698\n",
            "time to get object:  11.801607847213745\n",
            "time to get object:  12.759296178817749\n",
            "time to get object:  12.58600640296936\n",
            "time to get object:  11.786357164382935\n",
            "time to get object:  12.829991817474365\n",
            "time to get object:  12.736959457397461\n",
            "time to get object:  11.959690809249878\n",
            "time to get object:  12.604026317596436\n",
            "time to get object:  13.202010869979858\n",
            "time to get object:  11.816123723983765\n",
            "time to get object:  12.412957429885864\n",
            "time to get object:  12.266171932220459\n",
            "time to get object:  11.557307004928589\n",
            "time to get object:  11.708009719848633\n",
            "time to get object:  11.880220413208008\n",
            "time to get object:  12.37514877319336\n",
            "time to get object:  13.551928281784058\n",
            "time to get object:  11.6867995262146\n",
            "time to get object:  14.115406513214111\n",
            "time to get object:  11.564786195755005\n",
            "time to get object:  11.649138689041138\n",
            "time to get object:  11.883484840393066\n",
            "time to get object:  12.706726312637329\n",
            "time to get object:  11.39458703994751\n",
            "time to get object:  12.400794267654419\n",
            "time to get object:  12.20189356803894\n",
            "time to get object:  14.63191294670105\n",
            "time to get object:  11.839060544967651\n",
            "time to get object:  12.091055631637573\n",
            "time to get object:  11.679104089736938\n",
            "time to get object:  12.479628801345825\n",
            "time to get object:  11.818615198135376\n",
            "time to get object:  13.178005456924438\n",
            "time to get object:  12.761478662490845\n",
            "time to get object:  10.252109289169312\n",
            "time to get object:  11.883214235305786\n",
            "time to get object:  12.935531616210938\n",
            "time to get object:  13.731926918029785\n",
            "time to get object:  12.04721212387085\n",
            "time to get object:  11.94122314453125\n",
            "time to get object:  12.37975287437439\n",
            "time to get object:  12.676663637161255\n",
            "time to get object:  10.917733669281006\n",
            "time to get object:  14.51843547821045\n",
            "time to get object:  11.634634017944336\n",
            "time to get object:  12.502595663070679\n",
            "time to get object:  12.489882707595825\n",
            "time to get object:  12.553825855255127\n",
            "time to get object:  10.163743734359741\n",
            "time to get object:  14.32474684715271\n",
            "time to get object:  12.643643379211426\n",
            "time to get object:  11.739469766616821\n",
            "time to get object:  13.166645288467407\n",
            "time to get object:  11.62944507598877\n",
            "time to get object:  11.468427896499634\n",
            "time to get object:  12.496646642684937\n",
            "time to get object:  12.570706129074097\n",
            "time to get object:  11.598905563354492\n",
            "time to get object:  13.595449447631836\n",
            "time to get object:  12.412960290908813\n",
            "time to get object:  11.213470697402954\n",
            "time to get object:  13.627929925918579\n",
            "time to get object:  13.300633192062378\n",
            "time to get object:  11.51472020149231\n",
            "time to get object:  11.515627384185791\n",
            "time to get object:  13.954898595809937\n",
            "time to get object:  11.585108041763306\n",
            "time to get object:  10.637798309326172\n",
            "time to get object:  14.06367540359497\n",
            "time to get object:  12.194270849227905\n",
            "time to get object:  11.921971559524536\n",
            "time to get object:  11.929780721664429\n",
            "time to get object:  11.415822267532349\n",
            "time to get object:  12.554254531860352\n",
            "time to get object:  10.995656728744507\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class_names = train_ds.class_names\n",
        "gray_imgs = []\n",
        "plt.figure(figsize=(10, 10))\n",
        "for images, labels in train_ds_preprocessed.take(1):\n",
        "    for i in range(9):\n",
        "        ax = plt.subplot(3, 3, i + 1)\n",
        "        current_img = images[i]\n",
        "        plt.imshow(current_img.numpy()[:,:,:-1])\n",
        "        gray_imgs.append((current_img,np.argmax(labels[i])))\n",
        "        plt.title(class_names[np.argmax(labels[i])])\n",
        "        plt.axis(\"off\")\n",
        "plt.figure(figsize=(10, 10))\n",
        "for images, labels in train_ds_preprocessed.take(1):\n",
        "    for i in range(9):\n",
        "        ax = plt.subplot(3, 3, i + 1)\n",
        "        current_img = images[i]\n",
        "        plt.imshow(current_img.numpy()[:,:,-1],cmap=\"gray\")\n",
        "        gray_imgs.append((current_img,np.argmax(labels[i])))\n",
        "        plt.title(class_names[np.argmax(labels[i])])\n",
        "        plt.axis(\"off\")"
      ],
      "metadata": {
        "id": "Gadqgz3jJFfD"
      },
      "id": "Gadqgz3jJFfD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0265e93d",
      "metadata": {
        "id": "0265e93d"
      },
      "outputs": [],
      "source": [
        "#Visualizing gray images and other effects\n",
        "np.random.seed(0)\n",
        "\n",
        "def img_transforms_no_blur(data,label):\n",
        "    normalize_img = tf.keras.layers.Rescaling(1./255)\n",
        "    color_norm = normalize_img(data)\n",
        "    gray_img = tf.image.rgb_to_grayscale(data)\n",
        "    gray_norm_img = normalize_img(gray_img)\n",
        "    concat_img = tf.concat([color_norm, gray_norm_img],axis=-1)\n",
        "    return concat_img,label\n",
        "\n",
        "val_ds_preprocessed_no_blur = val_ds.map(img_transforms_no_blur)\n",
        "\n",
        "class_names = train_ds.class_names\n",
        "gray_imgs = []\n",
        "plt.figure(figsize=(10, 10))\n",
        "for images, labels in val_ds_preprocessed_no_blur.take(1):\n",
        "    for i in range(9):\n",
        "        ax = plt.subplot(3, 3, i + 1)\n",
        "        current_img = images[i]\n",
        "        plt.imshow(current_img.numpy()[:,:,-1],cmap=\"gray\")\n",
        "        gray_imgs.append((current_img,np.argmax(labels[i])))\n",
        "        plt.title(class_names[np.argmax(labels[i])])\n",
        "        plt.axis(\"off\")\n",
        "\n",
        "plt.figure(figsize=(10, 10))\n",
        "for i, (images, labels) in enumerate(gray_imgs):\n",
        "      ax = plt.subplot(3, 3, i + 1)\n",
        "      blurred_img = tfa.image.gaussian_filter2d(images[:,:,-1:],filter_shape=(10,10),sigma=5.0)\n",
        "      plt.imshow(blurred_img.numpy().squeeze(),cmap=\"gray\")\n",
        "      plt.title(class_names[labels])\n",
        "      plt.axis(\"off\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dual Stream ResNet Architecture"
      ],
      "metadata": {
        "id": "UmSResCstPc0"
      },
      "id": "UmSResCstPc0"
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.backend import dropout\n",
        "class DualStream_ResNet(tf.keras.Model):\n",
        "    def __init__(self, n_outputs, batch_size, freeze = True, load_type=None, \n",
        "                 merge_type=\"averaging\"):\n",
        "        super().__init__()\n",
        "        self.n_outputs =  n_outputs\n",
        "        self.merge_type = merge_type\n",
        "\n",
        "        self.magno_stream = tf.keras.applications.ResNet50(\n",
        "                            include_top=False,\n",
        "                            weights= load_type,\n",
        "                            input_tensor=None,\n",
        "                            input_shape=None,\n",
        "                            pooling=None,\n",
        "                            classes=n_outputs,\n",
        "                            )\n",
        "        \n",
        "        self.parvo_stream = tf.keras.applications.ResNet50(\n",
        "                            include_top=False,\n",
        "                            weights= load_type,\n",
        "                            input_tensor=None,\n",
        "                            input_shape=None,\n",
        "                            pooling=None,\n",
        "                            classes=n_outputs,\n",
        "                            )\n",
        "        if freeze:\n",
        "          self.magno_stream.trainable = False\n",
        "          self.parvo_stream.trainable = False\n",
        "\n",
        "        self.flat = tf.keras.layers.Flatten()\n",
        "        self.fc = Sequential([#tf.keras.layers.Dense(2048,activation='ReLU'),\n",
        "                              # tf.keras.layers.Dropout(0.5),\n",
        "                              tf.keras.layers.Dense(1280, activation='ReLU'),\n",
        "                              tf.keras.layers.Dropout(0.5),\n",
        "                              tf.keras.layers.Dense(640, activation='ReLU'),\n",
        "                              tf.keras.layers.Dropout(0.5),\n",
        "                              tf.keras.layers.Dense(self.n_outputs, activation='softmax')])\n",
        "        \n",
        "        self.avg = tf.keras.layers.Average()\n",
        "        self.concat = tf.keras.layers.Concatenate()\n",
        "\n",
        "    def call(self,inputs):\n",
        "        color_input = inputs[:,:,:,:-1]\n",
        "        gray_input = inputs[:,:,:,-1:]\n",
        "        concat_gray_input = tf.concat([gray_input,\n",
        "                                      gray_input,\n",
        "                                      gray_input],axis=-1)\n",
        "\n",
        "        m_stream = self.magno_stream(color_input)\n",
        "        m_stream = self.flat(m_stream)\n",
        "        \n",
        "        p_stream = self.parvo_stream(concat_gray_input)\n",
        "        p_stream = self.flat(p_stream)\n",
        "        \n",
        "        if self.merge_type == \"averaging\":\n",
        "          avg_outputs = self.avg([p_stream,m_stream])\n",
        "        elif self.merge_type == \"concat\":\n",
        "          avg_outputs = self.concat([p_stream,m_stream])\n",
        "        else:\n",
        "          raise \"That layer option is not available\"\n",
        "        return self.fc(avg_outputs)"
      ],
      "metadata": {
        "id": "PVDAyYHHiwX8"
      },
      "id": "PVDAyYHHiwX8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "FREEZE = False \n",
        "LOAD_WEIGHTS = None #One of None or imagenet\n",
        "dual_stream_model = DualStream_ResNet(10,batch_size,FREEZE,load_type=LOAD_WEIGHTS,\n",
        "                                      merge_type=\"averaging\")\n",
        "dual_stream_model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])"
      ],
      "metadata": {
        "id": "nrmij0NKjg0Z"
      },
      "id": "nrmij0NKjg0Z",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_save_loc = \"/content/drive/MyDrive/Vision Dataset/Models/\"\n",
        "model_name = \"Dualstream_with_ResNet_1000_Samples_Deeper_10Classes\"\n",
        "checkpoint_path = f\"{model_save_loc}{model_name}\"\n",
        "callback = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=20, restore_best_weights=True)\n",
        "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
        "                                                 monitor='val_accuracy',\n",
        "                                                 save_weights_only=True,\n",
        "                                                 verbose=1)\n",
        "\n",
        "train_history = dual_stream_model.fit(train_ds_preprocessed, validation_data= val_ds_preprocessed,\n",
        "                                      epochs= 60, callbacks=[callback,cp_callback], workers = 4)\n",
        "\n",
        "# dual_stream_model.save()\n",
        "\n",
        "#Save training history to CSV\n",
        "hist_df = pd.DataFrame(train_history.history)\n",
        "\n",
        "hist_csv_file = f\"{model_save_loc}{model_name}_history.csv\"\n",
        "with open(hist_csv_file, mode='w') as f:\n",
        "    hist_df.to_csv(f)\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(train_history.history['accuracy'])\n",
        "plt.plot(train_history.history['val_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'validation'], loc='upper left')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "RAojac_FjiHf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 831
        },
        "outputId": "0cb7c4f2-0c5a-4a1b-8517-dcc634c38be1"
      },
      "id": "RAojac_FjiHf",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/60\n",
            "1502/1502 [==============================] - ETA: 0s - loss: 5.0708 - accuracy: 0.0409\n",
            "Epoch 1: saving model to /content/drive/MyDrive/School/MIT Spring2022/9.60/Vision Project/Vision Dataset/Models/Dualstream_with_ResNet_1000_Samples_Deeper_50Classes\n",
            "1502/1502 [==============================] - 598s 391ms/step - loss: 5.0708 - accuracy: 0.0409 - val_loss: 3.7750 - val_accuracy: 0.0769\n",
            "Epoch 2/60\n",
            "1502/1502 [==============================] - ETA: 0s - loss: 3.3881 - accuracy: 0.1164\n",
            "Epoch 2: saving model to /content/drive/MyDrive/School/MIT Spring2022/9.60/Vision Project/Vision Dataset/Models/Dualstream_with_ResNet_1000_Samples_Deeper_50Classes\n",
            "1502/1502 [==============================] - 586s 390ms/step - loss: 3.3881 - accuracy: 0.1164 - val_loss: 3.1850 - val_accuracy: 0.1691\n",
            "Epoch 3/60\n",
            "1502/1502 [==============================] - ETA: 0s - loss: 3.0746 - accuracy: 0.1818\n",
            "Epoch 3: saving model to /content/drive/MyDrive/School/MIT Spring2022/9.60/Vision Project/Vision Dataset/Models/Dualstream_with_ResNet_1000_Samples_Deeper_50Classes\n",
            "1502/1502 [==============================] - 587s 390ms/step - loss: 3.0746 - accuracy: 0.1818 - val_loss: 3.4412 - val_accuracy: 0.1357\n",
            "Epoch 4/60\n",
            "1502/1502 [==============================] - ETA: 0s - loss: 2.9510 - accuracy: 0.2124\n",
            "Epoch 4: saving model to /content/drive/MyDrive/School/MIT Spring2022/9.60/Vision Project/Vision Dataset/Models/Dualstream_with_ResNet_1000_Samples_Deeper_50Classes\n",
            "1502/1502 [==============================] - 586s 389ms/step - loss: 2.9510 - accuracy: 0.2124 - val_loss: 3.2394 - val_accuracy: 0.1756\n",
            "Epoch 5/60\n",
            "1502/1502 [==============================] - ETA: 0s - loss: 2.8083 - accuracy: 0.2439\n",
            "Epoch 5: saving model to /content/drive/MyDrive/School/MIT Spring2022/9.60/Vision Project/Vision Dataset/Models/Dualstream_with_ResNet_1000_Samples_Deeper_50Classes\n",
            "1502/1502 [==============================] - 588s 391ms/step - loss: 2.8083 - accuracy: 0.2439 - val_loss: 3.3271 - val_accuracy: 0.1580\n",
            "Epoch 6/60\n",
            "1502/1502 [==============================] - ETA: 0s - loss: 2.6308 - accuracy: 0.2822\n",
            "Epoch 6: saving model to /content/drive/MyDrive/School/MIT Spring2022/9.60/Vision Project/Vision Dataset/Models/Dualstream_with_ResNet_1000_Samples_Deeper_50Classes\n",
            "1502/1502 [==============================] - 586s 389ms/step - loss: 2.6308 - accuracy: 0.2822 - val_loss: 3.2805 - val_accuracy: 0.1655\n",
            "Epoch 7/60\n",
            " 265/1502 [====>.........................] - ETA: 8:07 - loss: 2.5626 - accuracy: 0.2996"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-f5d858239a3d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m train_history = dual_stream_model.fit(train_ds_preprocessed, validation_data= val_ds_preprocessed,\n\u001b[0;32m---> 11\u001b[0;31m                                       epochs= 60, callbacks=[callback,cp_callback], workers = 4)\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# dual_stream_model.save()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1387\u001b[0m               \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtmp_logs\u001b[0m  \u001b[0;31m# No error, now safe to assign to logs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1388\u001b[0m               \u001b[0mend_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_increment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1389\u001b[0;31m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1390\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    436\u001b[0m     \"\"\"\n\u001b[1;32m    437\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_call_train_batch_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 438\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'end'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    439\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_test_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook\u001b[0;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[1;32m    295\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_begin_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'end'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 297\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_end_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    298\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m       raise ValueError(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_end_hook\u001b[0;34m(self, mode, batch, logs)\u001b[0m\n\u001b[1;32m    316\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_times\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 318\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhook_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_times\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_batches_for_timing_check\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook_helper\u001b[0;34m(self, hook_name, batch, logs)\u001b[0m\n\u001b[1;32m    354\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m       \u001b[0mhook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 356\u001b[0;31m       \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    357\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_timing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1032\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1033\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1034\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_update_progbar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1035\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_test_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_batch_update_progbar\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1104\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1105\u001b[0m       \u001b[0;31m# Only block async when verbose = 1.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1106\u001b[0;31m       \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msync_to_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1107\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprogbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/tf_utils.py\u001b[0m in \u001b[0;36msync_to_numpy_or_python_type\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m    561\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 563\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_to_single_numpy_or_python_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    564\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    565\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    912\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    913\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 914\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    915\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    912\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    913\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 914\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    915\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/tf_utils.py\u001b[0m in \u001b[0;36m_to_single_numpy_or_python_type\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    555\u001b[0m     \u001b[0;31m# Don't turn ragged or sparse tensors to NumPy.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 557\u001b[0;31m       \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    558\u001b[0m     \u001b[0;31m# Strings, ragged and sparse tensors don't have .item(). Return them as-is.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneric\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mnumpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1221\u001b[0m     \"\"\"\n\u001b[1;32m   1222\u001b[0m     \u001b[0;31m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1223\u001b[0;31m     \u001b[0mmaybe_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1224\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaybe_arr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1187\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1188\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1189\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1190\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1191\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Load Model:\n",
        "dual_stream_model = tf.keras.models.load_model(\"/content/drive/MyDrive/School/MIT Spring2022/9.60/Vision Project/Vision Dataset/Models/Dualstream_with_ResNet_1000_Samples_Deeper_50Classes\")\n",
        "# eval_loss, eval_accuracy = dual_stream_model.evaluate(val_ds_preprocessed)\n"
      ],
      "metadata": {
        "id": "DgvAWYPbviCb"
      },
      "id": "DgvAWYPbviCb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Simple ResNet architectur test"
      ],
      "metadata": {
        "id": "xoNAxmHW6CrQ"
      },
      "id": "xoNAxmHW6CrQ"
    },
    {
      "cell_type": "code",
      "source": [
        "#Testing Pure ResNet Model Training\n",
        "from keras.backend import dropout\n",
        "class Single_ResNet(tf.keras.Model):\n",
        "    def __init__(self, n_outputs, batch_size, freeze = True, load_type=None, \n",
        "                 merge_type=\"averaging\"):\n",
        "        super().__init__()\n",
        "        self.n_outputs =  n_outputs\n",
        "        self.merge_type = merge_type\n",
        "\n",
        "        self.magno_stream = tf.keras.applications.ResNet50(\n",
        "                            include_top=False,\n",
        "                            weights= load_type,\n",
        "                            input_tensor=None,\n",
        "                            input_shape=None,\n",
        "                            pooling=None,\n",
        "                            classes=n_outputs,\n",
        "                            )\n",
        "        if freeze:\n",
        "          self.magno_stream.trainable = False\n",
        "\n",
        "        self.flat = tf.keras.layers.Flatten()\n",
        "        self.fc = Sequential([#tf.keras.layers.Dense(2048,activation='ReLU'),\n",
        "                              #tf.keras.layers.Dropout(0.5),\n",
        "                              tf.keras.layers.Dense(1280, activation='ReLU'),\n",
        "                              tf.keras.layers.Dropout(0.5),\n",
        "                              tf.keras.layers.Dense(640, activation='ReLU'),\n",
        "                              tf.keras.layers.Dropout(0.5),\n",
        "                              tf.keras.layers.Dense(self.n_outputs, activation='softmax')])\n",
        "        \n",
        "        self.avg = tf.keras.layers.Average()\n",
        "        self.concat = tf.keras.layers.Concatenate()\n",
        "\n",
        "    def call(self,inputs):\n",
        "        color_input = inputs[:,:,:,:-1]\n",
        "\n",
        "        m_stream = self.magno_stream(color_input)\n",
        "        m_stream = self.flat(m_stream)\n",
        "\n",
        "        return self.fc(m_stream)"
      ],
      "metadata": {
        "id": "6Q0i4qDtJ1mx"
      },
      "id": "6Q0i4qDtJ1mx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "FREEZE = False \n",
        "LOAD_WEIGHTS = None #One of None or imagenet\n",
        "signle_resnet_model = Single_ResNet(10,batch_size,FREEZE,load_type=LOAD_WEIGHTS,\n",
        "                                      merge_type=\"averaging\")\n",
        "signle_resnet_model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
        "\n",
        "callback = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=20, restore_best_weights=True)\n",
        "\n",
        "train_history = signle_resnet_model.fit(train_ds_preprocessed, validation_data= val_ds_preprocessed,\n",
        "                                      epochs= 30, callbacks=callback, workers = 4)\n",
        "signle_resnet_model.save(\"/content/drive/MyDrive/School/MIT Spring2022/9.60/Vision Project/Vision Dataset/Models/Single_ResNet_1000_Samples_Deeper\")"
      ],
      "metadata": {
        "id": "n82_rX4HgSEg",
        "outputId": "202b614d-7f4a-4d1b-ac1e-68a97a4280ec",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "n82_rX4HgSEg",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "291/291 [==============================] - 72s 227ms/step - loss: 13.6862 - accuracy: 0.1110 - val_loss: 2.3041 - val_accuracy: 0.1156\n",
            "Epoch 2/30\n",
            "291/291 [==============================] - 64s 219ms/step - loss: 2.2606 - accuracy: 0.1448 - val_loss: 2.2146 - val_accuracy: 0.1586\n",
            "Epoch 3/30\n",
            "291/291 [==============================] - 64s 219ms/step - loss: 2.2257 - accuracy: 0.1625 - val_loss: 2.1758 - val_accuracy: 0.1844\n",
            "Epoch 4/30\n",
            "291/291 [==============================] - 63s 216ms/step - loss: 2.1793 - accuracy: 0.1795 - val_loss: 2.1855 - val_accuracy: 0.1624\n",
            "Epoch 5/30\n",
            "291/291 [==============================] - 64s 220ms/step - loss: 2.1579 - accuracy: 0.1872 - val_loss: 2.0857 - val_accuracy: 0.1990\n",
            "Epoch 6/30\n",
            "291/291 [==============================] - 64s 220ms/step - loss: 2.1143 - accuracy: 0.2057 - val_loss: 2.0430 - val_accuracy: 0.2119\n",
            "Epoch 7/30\n",
            "291/291 [==============================] - 64s 219ms/step - loss: 2.0276 - accuracy: 0.2475 - val_loss: 1.8438 - val_accuracy: 0.3025\n",
            "Epoch 8/30\n",
            "291/291 [==============================] - 64s 220ms/step - loss: 1.8449 - accuracy: 0.3208 - val_loss: 1.7195 - val_accuracy: 0.3610\n",
            "Epoch 9/30\n",
            "291/291 [==============================] - 64s 219ms/step - loss: 1.7053 - accuracy: 0.3706 - val_loss: 1.7697 - val_accuracy: 0.3876\n",
            "Epoch 10/30\n",
            "291/291 [==============================] - 64s 220ms/step - loss: 1.5818 - accuracy: 0.4107 - val_loss: 1.6281 - val_accuracy: 0.4302\n",
            "Epoch 11/30\n",
            "291/291 [==============================] - 64s 220ms/step - loss: 1.4487 - accuracy: 0.4666 - val_loss: 1.5069 - val_accuracy: 0.4968\n",
            "Epoch 12/30\n",
            "291/291 [==============================] - 64s 219ms/step - loss: 1.3576 - accuracy: 0.5206 - val_loss: 1.4353 - val_accuracy: 0.5131\n",
            "Epoch 13/30\n",
            "291/291 [==============================] - 63s 216ms/step - loss: 1.2633 - accuracy: 0.5657 - val_loss: 1.6752 - val_accuracy: 0.4663\n",
            "Epoch 14/30\n",
            "291/291 [==============================] - 63s 216ms/step - loss: 1.1621 - accuracy: 0.6034 - val_loss: 1.5462 - val_accuracy: 0.4985\n",
            "Epoch 15/30\n",
            "291/291 [==============================] - 64s 219ms/step - loss: 1.0607 - accuracy: 0.6402 - val_loss: 1.3759 - val_accuracy: 0.5496\n",
            "Epoch 16/30\n",
            "291/291 [==============================] - 64s 220ms/step - loss: 0.9723 - accuracy: 0.6715 - val_loss: 1.3965 - val_accuracy: 0.5505\n",
            "Epoch 17/30\n",
            "291/291 [==============================] - 64s 220ms/step - loss: 0.9068 - accuracy: 0.7002 - val_loss: 1.1815 - val_accuracy: 0.6068\n",
            "Epoch 18/30\n",
            "291/291 [==============================] - 63s 216ms/step - loss: 0.9139 - accuracy: 0.7037 - val_loss: 1.9005 - val_accuracy: 0.4276\n",
            "Epoch 19/30\n",
            "291/291 [==============================] - 63s 216ms/step - loss: 0.8613 - accuracy: 0.7239 - val_loss: 1.6165 - val_accuracy: 0.5488\n",
            "Epoch 20/30\n",
            "291/291 [==============================] - 64s 220ms/step - loss: 0.9083 - accuracy: 0.7168 - val_loss: 0.8941 - val_accuracy: 0.7065\n",
            "Epoch 21/30\n",
            "291/291 [==============================] - 63s 216ms/step - loss: 0.8253 - accuracy: 0.7427 - val_loss: 2.4588 - val_accuracy: 0.4031\n",
            "Epoch 22/30\n",
            "291/291 [==============================] - 63s 216ms/step - loss: 0.7770 - accuracy: 0.7578 - val_loss: 1.5404 - val_accuracy: 0.5526\n",
            "Epoch 23/30\n",
            "291/291 [==============================] - 63s 216ms/step - loss: 0.7173 - accuracy: 0.7754 - val_loss: 1.4024 - val_accuracy: 0.5806\n",
            "Epoch 24/30\n",
            "291/291 [==============================] - 63s 216ms/step - loss: 0.7621 - accuracy: 0.7703 - val_loss: 2.4900 - val_accuracy: 0.4968\n",
            "Epoch 25/30\n",
            "291/291 [==============================] - 63s 216ms/step - loss: 0.6221 - accuracy: 0.7949 - val_loss: 1.5285 - val_accuracy: 0.5698\n",
            "Epoch 26/30\n",
            "291/291 [==============================] - 63s 216ms/step - loss: 0.6679 - accuracy: 0.7931 - val_loss: 1.4029 - val_accuracy: 0.6768\n",
            "Epoch 27/30\n",
            "291/291 [==============================] - 63s 216ms/step - loss: 0.7229 - accuracy: 0.7907 - val_loss: 1.6352 - val_accuracy: 0.5471\n",
            "Epoch 28/30\n",
            "291/291 [==============================] - 63s 216ms/step - loss: 0.6196 - accuracy: 0.8157 - val_loss: 5.3809 - val_accuracy: 0.5191\n",
            "Epoch 29/30\n",
            "291/291 [==============================] - 63s 216ms/step - loss: 0.5893 - accuracy: 0.8316 - val_loss: 2.4090 - val_accuracy: 0.5724\n",
            "Epoch 30/30\n",
            "291/291 [==============================] - 64s 220ms/step - loss: 0.4971 - accuracy: 0.8489 - val_loss: 1.0312 - val_accuracy: 0.7099\n",
            "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.merge.Average object at 0x7fabf524acd0>, because it is not built.\n",
            "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.merge.Concatenate object at 0x7faa38845810>, because it is not built.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as re_lu_6_layer_call_fn, re_lu_6_layer_call_and_return_conditional_losses, re_lu_7_layer_call_fn, re_lu_7_layer_call_and_return_conditional_losses while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/School/MIT Spring2022/9.60/Vision Project/Vision Dataset/Models/Single_ResNet_1000_Samples_Deeper/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/School/MIT Spring2022/9.60/Vision Project/Vision Dataset/Models/Single_ResNet_1000_Samples_Deeper/assets\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DualStream with VGG versions (Pending)"
      ],
      "metadata": {
        "id": "tvD1k9Kg2E14"
      },
      "id": "tvD1k9Kg2E14"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DualStream EfficientNet Architecture\n",
        "(limitation: it overfits really badly on our current dataset. The literature reports overfitting when smaller training sets are given as a serious issue with EffectiveNet)"
      ],
      "metadata": {
        "id": "I4kX10nytVHV"
      },
      "id": "I4kX10nytVHV"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a3258579",
      "metadata": {
        "id": "a3258579"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "                                                                                \n",
        "class DualStream_ENet(tf.keras.Model):\n",
        "    def __init__(self, n_outputs, batch_size, freeze = True, load_type=None,\n",
        "                 merge_type=\"averaging\"):\n",
        "        super().__init__()\n",
        "        self.n_outputs =  n_outputs\n",
        "        self.merge_type = merge_type\n",
        "\n",
        "        self.magno_stream = tf.keras.applications.EfficientNetB0(\n",
        "                            include_top=False,\n",
        "                            weights= load_type,\n",
        "                            input_tensor=None,\n",
        "                            input_shape=None,\n",
        "                            pooling=None,\n",
        "                            classes=self.n_outputs,\n",
        "                            classifier_activation=\"softmax\",\n",
        "                            )\n",
        "        \n",
        "        self.parvo_stream = tf.keras.applications.EfficientNetB0(\n",
        "                            include_top=False,\n",
        "                            weights= load_type,\n",
        "                            input_tensor=None,\n",
        "                            input_shape=None,\n",
        "                            pooling=None,\n",
        "                            classes=self.n_outputs,\n",
        "                            classifier_activation=\"softmax\",\n",
        "                            )\n",
        "        if freeze:\n",
        "          self.magno_stream.trainable = False\n",
        "          self.parvo_stream.trainable = False\n",
        "\n",
        "        self.flat = tf.keras.layers.Flatten()\n",
        "        self.fc = Sequential([tf.keras.layers.Dense(2048,activation='ReLU'),\n",
        "                              tf.keras.layers.Dropout(0.5),\n",
        "                              tf.keras.layers.Dense(1280, activation='ReLU'),\n",
        "                              tf.keras.layers.Dropout(0.5),\n",
        "                              tf.keras.layers.Dense(640, activation='ReLU'),\n",
        "                              tf.keras.layers.Dropout(0.5),\n",
        "                              tf.keras.layers.Dense(self.n_outputs, activation='softmax')])\n",
        "        \n",
        "        self.avg = tf.keras.layers.Average()\n",
        "        self.concat = tf.keras.layers.Concatenate()\n",
        "        self.fc_out = tf.keras.layers.Dense(self.n_outputs, activation='softmax')\n",
        "\n",
        "    def call(self,inputs):\n",
        "        color_input = inputs[:,:,:,:-1]\n",
        "        # start_time = time.time()\n",
        "        gray_input = inputs[:,:,:,-1:]\n",
        "        concat_gray_input = tf.concat([gray_input,\n",
        "                                      gray_input,\n",
        "                                      gray_input],axis=-1)\n",
        "        # print(\"delta_time:\",time.time()-start_time)\n",
        "\n",
        "        m_stream = self.magno_stream(color_input)\n",
        "        m_stream = self.flat(m_stream)\n",
        "        \n",
        "        p_stream = self.parvo_stream(concat_gray_input)\n",
        "        p_stream = self.flat(p_stream)\n",
        "        \n",
        "        if self.merge_type == \"averaging\":\n",
        "          avg_outputs = self.avg([p_stream,m_stream])\n",
        "        elif self.merge_type == \"concat\":\n",
        "          avg_outputs = self.concat([p_stream,m_stream])\n",
        "        else:\n",
        "          raise \"That layer option is not available\"\n",
        "        return self.fc(avg_outputs)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "FREEZE = False #One of None or imagenet\n",
        "dual_stream_model = DualStream_ENet(50,batch_size,FREEZE,load_type=\"imagenet\",\n",
        "                                    merge_type=\"averaging\")\n",
        "dual_stream_model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])"
      ],
      "metadata": {
        "id": "i9xe3HaOoISJ"
      },
      "id": "i9xe3HaOoISJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# checkpoint_path = \"/content/drive/MyDrive/School/MIT Spring2022/9.60/Vision Project/Models//content/drive/MyDrive/School/MIT Spring2022/9.60/Vision Project/Models/Dualstream_with_ResNet2.0\"\n",
        "callback = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=20, restore_best_weights=True)\n",
        "# cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
        "#                                                  save_weights_only=True,\n",
        "#                                                  verbose=1)\n",
        "\n",
        "train_history = dual_stream_model.fit(train_ds_preprocessed, validation_data= val_ds_preprocessed,\n",
        "                                      epochs= 30, callbacks=callback, workers = 4)\n",
        "dual_stream_model.save(\"/content/drive/MyDrive/School/MIT Spring2022/9.60/Vision Project/Models/Dualstream_with_EfficientNet_Deeper\")"
      ],
      "metadata": {
        "id": "bd3TM4OZquYo",
        "outputId": "6b7bba78-84c2-45fa-937c-0b1cce175de1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "bd3TM4OZquYo",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "489/489 [==============================] - 348s 671ms/step - loss: 2.9101 - accuracy: 0.3179 - val_loss: 5.7237 - val_accuracy: 0.0133\n",
            "Epoch 2/30\n",
            "489/489 [==============================] - 325s 663ms/step - loss: 2.3374 - accuracy: 0.4244 - val_loss: 19.3927 - val_accuracy: 0.0190\n",
            "Epoch 3/30\n",
            "489/489 [==============================] - 325s 663ms/step - loss: 2.2102 - accuracy: 0.4646 - val_loss: 44.7883 - val_accuracy: 0.0207\n",
            "Epoch 4/30\n",
            "489/489 [==============================] - 325s 664ms/step - loss: 2.2732 - accuracy: 0.4535 - val_loss: 13.0702 - val_accuracy: 0.0228\n",
            "Epoch 5/30\n",
            "489/489 [==============================] - 324s 662ms/step - loss: 2.1662 - accuracy: 0.4679 - val_loss: 4.0507 - val_accuracy: 0.0251\n",
            "Epoch 6/30\n",
            "489/489 [==============================] - 324s 661ms/step - loss: 2.1699 - accuracy: 0.4697 - val_loss: 136.9688 - val_accuracy: 0.0182\n",
            "Epoch 7/30\n",
            "489/489 [==============================] - 324s 662ms/step - loss: 2.3259 - accuracy: 0.4434 - val_loss: 5.0203 - val_accuracy: 0.0269\n",
            "Epoch 8/30\n",
            "489/489 [==============================] - 324s 662ms/step - loss: 2.1509 - accuracy: 0.4663 - val_loss: 4.4336 - val_accuracy: 0.0297\n",
            "Epoch 9/30\n",
            "489/489 [==============================] - 324s 661ms/step - loss: 2.1614 - accuracy: 0.4771 - val_loss: 211.7433 - val_accuracy: 0.0202\n",
            "Epoch 10/30\n",
            "489/489 [==============================] - 324s 661ms/step - loss: 2.0761 - accuracy: 0.4886 - val_loss: 6.5694 - val_accuracy: 0.0256\n",
            "Epoch 11/30\n",
            "489/489 [==============================] - 323s 659ms/step - loss: 2.2387 - accuracy: 0.4535 - val_loss: 5.1734 - val_accuracy: 0.0190\n",
            "Epoch 12/30\n",
            "489/489 [==============================] - 324s 661ms/step - loss: 2.2110 - accuracy: 0.4583 - val_loss: 3.9455 - val_accuracy: 0.0294\n",
            "Epoch 13/30\n",
            "489/489 [==============================] - 323s 661ms/step - loss: 2.1322 - accuracy: 0.4773 - val_loss: 6.6927 - val_accuracy: 0.0143\n",
            "Epoch 14/30\n",
            "489/489 [==============================] - 323s 661ms/step - loss: 2.1872 - accuracy: 0.4592 - val_loss: 5.9065 - val_accuracy: 0.0187\n",
            "Epoch 15/30\n",
            "489/489 [==============================] - 324s 661ms/step - loss: 2.2838 - accuracy: 0.4471 - val_loss: 4.5453 - val_accuracy: 0.0136\n",
            "Epoch 16/30\n",
            "489/489 [==============================] - 325s 665ms/step - loss: 2.2285 - accuracy: 0.4513 - val_loss: 4.0267 - val_accuracy: 0.0676\n",
            "Epoch 17/30\n",
            "489/489 [==============================] - 322s 658ms/step - loss: 2.2852 - accuracy: 0.4375 - val_loss: 22.4893 - val_accuracy: 0.0179\n",
            "Epoch 18/30\n",
            "489/489 [==============================] - 323s 661ms/step - loss: 2.4089 - accuracy: 0.4031 - val_loss: 6.4417 - val_accuracy: 0.0197\n",
            "Epoch 19/30\n",
            "489/489 [==============================] - 323s 660ms/step - loss: 2.3084 - accuracy: 0.4389 - val_loss: 1444.3357 - val_accuracy: 0.0179\n",
            "Epoch 20/30\n",
            "324/489 [==================>...........] - ETA: 1:44 - loss: 2.4381 - accuracy: 0.4117"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dual Stream VGG Net"
      ],
      "metadata": {
        "id": "53LvddVzO0KK"
      },
      "id": "53LvddVzO0KK"
    },
    {
      "cell_type": "code",
      "source": [
        "class DualStream_VGG(tf.keras.Model):\n",
        "    def __init__(self, n_outputs, batch_size, merge_type=\"averaging\"):\n",
        "        super().__init__()\n",
        "        self.n_outputs =  n_outputs\n",
        "        self.merge_type = merge_type\n",
        "\n",
        "        self.magno_stream = Sequential([\n",
        "        Conv2D(input_shape=(224,224,3),filters=64,kernel_size=(3,3),padding=\"same\", activation=\"relu\"),\n",
        "        Conv2D(filters=64,kernel_size=(3,3),padding=\"same\", activation=\"relu\"),\n",
        "        MaxPool2D(pool_size=(2,2),strides=(2,2)),\n",
        "        Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\"),\n",
        "        Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\"),\n",
        "        MaxPool2D(pool_size=(2,2),strides=(2,2)),\n",
        "        Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\"),\n",
        "        Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\"),\n",
        "        Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\"),\n",
        "        MaxPool2D(pool_size=(2,2),strides=(2,2)),\n",
        "        Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"),\n",
        "        Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"),\n",
        "        Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"),\n",
        "        MaxPool2D(pool_size=(2,2),strides=(2,2)),\n",
        "        Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"),\n",
        "        Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"),\n",
        "        Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"),\n",
        "        MaxPool2D(pool_size=(2,2),strides=(2,2)),\n",
        "        Flatten()\n",
        "        ])\n",
        "        \n",
        "        self.parvo_stream = Sequential([\n",
        "        Conv2D(input_shape=(224,224,1),filters=64,kernel_size=(3,3),padding=\"same\", activation=\"relu\"),\n",
        "        Conv2D(filters=64,kernel_size=(3,3),padding=\"same\", activation=\"relu\"),\n",
        "        MaxPool2D(pool_size=(2,2),strides=(2,2)),\n",
        "        Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\"),\n",
        "        Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\"),\n",
        "        MaxPool2D(pool_size=(2,2),strides=(2,2)),\n",
        "        Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\"),\n",
        "        Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\"),\n",
        "        Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\"),\n",
        "        MaxPool2D(pool_size=(2,2),strides=(2,2)),\n",
        "        Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"),\n",
        "        Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"),\n",
        "        Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"),\n",
        "        MaxPool2D(pool_size=(2,2),strides=(2,2)),\n",
        "        Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"),\n",
        "        Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"),\n",
        "        Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"),\n",
        "        MaxPool2D(pool_size=(2,2),strides=(2,2)),\n",
        "        Flatten()\n",
        "        ])\n",
        "\n",
        "        self.flat = tf.keras.layers.Flatten()\n",
        "        self.fc = Sequential([#tf.keras.layers.Dense(2048,activation='ReLU'),\n",
        "                              # tf.keras.layers.Dropout(0.5),\n",
        "                              # tf.keras.layers.Dense(1280, activation='ReLU'),\n",
        "                              # tf.keras.layers.Dropout(0.5),\n",
        "                              # tf.keras.layers.Dense(640, activation='ReLU'),\n",
        "                              # tf.keras.layers.Dropout(0.5),\n",
        "                              tf.keras.layers.Dense(self.n_outputs, activation='softmax')])\n",
        "        \n",
        "        self.avg = tf.keras.layers.Average()\n",
        "        self.concat = tf.keras.layers.Concatenate()\n",
        "\n",
        "    def call(self,inputs):\n",
        "        color_input = inputs[:,:,:,:-1]\n",
        "        gray_input = inputs[:,:,:,-1:]\n",
        "\n",
        "        m_stream = self.magno_stream(color_input)\n",
        "        m_stream = self.flat(m_stream)\n",
        "        \n",
        "        p_stream = self.parvo_stream(gray_input)\n",
        "        p_stream = self.flat(p_stream)\n",
        "        \n",
        "        if self.merge_type == \"averaging\":\n",
        "          avg_outputs = self.avg([p_stream,m_stream])\n",
        "        elif self.merge_type == \"concat\":\n",
        "          avg_outputs = self.concat([p_stream,m_stream])\n",
        "        else:\n",
        "          raise \"That layer option is not available\"\n",
        "        return self.fc(avg_outputs)"
      ],
      "metadata": {
        "id": "jXT0SNiJOf6H"
      },
      "id": "jXT0SNiJOf6H",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dual_stream_model = DualStream_VGG(10,batch_size,merge_type=\"averaging\")\n",
        "dual_stream_model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
        "\n",
        "model_save_loc = \"/content/drive/MyDrive/Vision Dataset/Models/\"\n",
        "model_name = \"Dualstream_with_VGG_1000_Samples_10Classes\"\n",
        "checkpoint_path = f\"{model_save_loc}{model_name}\"\n",
        "callback = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=20, restore_best_weights=True)\n",
        "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
        "                                                 monitor='val_accuracy',\n",
        "                                                 save_weights_only=True,\n",
        "                                                 verbose=1)\n",
        "\n",
        "train_history = dual_stream_model.fit(train_ds_preprocessed, validation_data= val_ds_preprocessed,\n",
        "                                      epochs= 60, callbacks=[callback,cp_callback], workers = 4)\n",
        "\n",
        "# dual_stream_model.save()\n",
        "\n",
        "#Save training history to CSV\n",
        "hist_df = pd.DataFrame(train_history.history)\n",
        "\n",
        "hist_csv_file = f\"{model_save_loc}{model_name}_history.csv\"\n",
        "with open(hist_csv_file, mode='w') as f:\n",
        "    hist_df.to_csv(f)\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(train_history.history['accuracy'])\n",
        "plt.plot(train_history.history['val_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'validation'], loc='upper left')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lmPr8E-HPyCm"
      },
      "id": "lmPr8E-HPyCm",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "Magno_parvo_algorithms.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}